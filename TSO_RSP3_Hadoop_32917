Last login: Fri Mar 24 22:59:09 on ttys000
brandon-mitchells-computer:~ bmitchell$ cd .ssh
brandon-mitchells-computer:.ssh bmitchell$ nano known_hosts
brandon-mitchells-computer:.ssh bmitchell$ ssh root@192.168.1.2
The authenticity of host '192.168.1.2 (192.168.1.2)' can't be established.
ECDSA key fingerprint is SHA256:X9AYaqaDXfK+bgisDppkqmeN9GPcW+gy/TDYxuKSMiQ.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.1.2' (ECDSA) to the list of known hosts.
root@192.168.1.2's password: 

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 00:32:44 2017 from 192.168.1.6
root@master:~# nano /etc/dhcpcd.conf
root@master:~# raspi-config
root@master:~# nano /etc/hostname
root@master:~# nano /etc/hosts
root@master:~# nano /opt/hadoop/etc/hadoop/slaves
root@master:~# nano /opt/hadoop/etc/hadoop/core-site.xml
root@master:~# nano /opt/hadoop/etc/hadoop/hdfs-site.xml
root@master:~# nano /opt/hadoop/etc/hadoop/yarn-site.xml
root@master:~# nano /opt/hadoop/etc/hadoop/mapred-site.xml
root@master:~# nano /opt/hadoop/etc/hadoop/mapred-site.xml
root@master:~# raspi-config
Connection to 192.168.1.2 closed by remote host.
Connection to 192.168.1.2 closed.
brandon-mitchells-computer:.ssh bmitchell$ ssh root@192.168.1.2
root@192.168.1.2's password: 

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 00:32:44 2017 from 192.168.1.6
root@master:~# nano /etc/dhcpcd.conf
root@master:~# nano /etc/hostname
root@master:~# nano /opt/hadoop/etc/hadoop/slaves
root@master:~# nano /opt/hadoop/etc/hadoop/core-site.xml
root@master:~# nano /opt/hadoop/etc/hadoop/hdfs-site.xml
root@master:~# nano /opt/hadoop/etc/hadoop/yarn-site.xml
root@master:~# nano /etc/hostname
root@master:~# raspi-config
Connection to 192.168.1.2 closed by remote host.
Connection to 192.168.1.2 closed.
brandon-mitchells-computer:.ssh bmitchell$ ssh root@192.168.1.2
root@192.168.1.2's password: 

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 00:32:44 2017 from 192.168.1.6
root@master:~# nano /opt/hadoop/etc/hadoop/mapred-site.xml
root@master:~# nano /opt/hadoop/etc/hadoop/yarn-site.xml
root@master:~# ssh root@192.168.1.3

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 00:35:54 2017 from master
root@slave0:~# nano /opt/hadoop/etc/hadoop/mapred-site.xml
root@slave0:~# exit
logout
Connection to 192.168.1.3 closed.
root@master:~# ssh root@192.168.1.4
The authenticity of host '192.168.1.4 (192.168.1.4)' can't be established.
ECDSA key fingerprint is 93:4f:3d:cd:32:d1:6e:6c:6e:20:05:fb:a3:9d:47:99.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.1.4' (ECDSA) to the list of known hosts.

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 03:50:32 2017 from 192.168.1.6
root@slave1:~# exit
logout
Connection to 192.168.1.4 closed.
root@master:~# ssh root@192.168.1.4

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:34:59 2017 from master
root@slave1:~# nano /opt/hadoop/etc/hadoop/mapred-site.xml
root@slave1:~# exit
logout
Connection to 192.168.1.4 closed.
root@master:~# ssh root@192.168.1.5
The authenticity of host '192.168.1.5 (192.168.1.5)' can't be established.
ECDSA key fingerprint is 93:4f:3d:cd:32:d1:6e:6c:6e:20:05:fb:a3:9d:47:99.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.1.5' (ECDSA) to the list of known hosts.

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:25:53 2017 from 192.168.1.6
root@slave2:~# exit
logout
Connection to 192.168.1.5 closed.
root@master:~# ssh root@192.168.1.5

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:49:31 2017 from master
root@slave2:~# nano /opt/hadoop/etc/hadoop/mapred-site.xml
root@slave2:~# exit
logout
Connection to 192.168.1.5 closed.
root@master:~# su hduser
hduser@master:/root $ cd ~
hduser@master:~ $ ssh hduser@192.168.1.3
The authenticity of host '192.168.1.3 (192.168.1.3)' can't be established.
ECDSA key fingerprint is 93:4f:3d:cd:32:d1:6e:6c:6e:20:05:fb:a3:9d:47:99.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.1.3' (ECDSA) to the list of known hosts.

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Sun Mar 26 05:38:51 2017 from localhost
hduser@slave0:~ $ exit
logout
Connection to 192.168.1.3 closed.
hduser@master:~ $ ssh hduser@192.168.1.3

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:51:18 2017 from master
hduser@slave0:~ $ exit
logout
Connection to 192.168.1.3 closed.
hduser@master:~ $ ssh hduser@192.168.1.4
The authenticity of host '192.168.1.4 (192.168.1.4)' can't be established.
ECDSA key fingerprint is 93:4f:3d:cd:32:d1:6e:6c:6e:20:05:fb:a3:9d:47:99.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.1.4' (ECDSA) to the list of known hosts.

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Sun Mar 26 05:38:51 2017 from localhost
hduser@slave1:~ $ exit
logout
Connection to 192.168.1.4 closed.
hduser@master:~ $ ssh hduser@192.168.1.4

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:51:38 2017 from master
hduser@slave1:~ $ exit
logout
Connection to 192.168.1.4 closed.
hduser@master:~ $ ssh hduser@192.168.1.5
The authenticity of host '192.168.1.5 (192.168.1.5)' can't be established.
ECDSA key fingerprint is 93:4f:3d:cd:32:d1:6e:6c:6e:20:05:fb:a3:9d:47:99.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.1.5' (ECDSA) to the list of known hosts.

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Sun Mar 26 05:38:51 2017 from localhost
hduser@slave2:~ $ exit
logout
Connection to 192.168.1.5 closed.
hduser@master:~ $ ssh hduser@192.168.1.5

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:51:51 2017 from master
hduser@slave2:~ $ exit
logout
Connection to 192.168.1.5 closed.
hduser@master:~ $ exit
exit
root@master:~# bash start-all.sh
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [master]
master: starting namenode, logging to /opt/hadoop/logs/hadoop-root-namenode-master.out
The authenticity of host 'slave3 (192.168.1.5)' can't be established.
ECDSA key fingerprint is 93:4f:3d:cd:32:d1:6e:6c:6e:20:05:fb:a3:9d:47:99.
Are you sure you want to continue connecting (yes/no)? The authenticity of host 'slave1 (192.168.1.3)' can't be established.
ECDSA key fingerprint is 93:4f:3d:cd:32:d1:6e:6c:6e:20:05:fb:a3:9d:47:99.
Are you sure you want to continue connecting (yes/no)? master: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-master.out
yes
slave3: Warning: Permanently added 'slave3' (ECDSA) to the list of known hosts.
slave3: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave2.out
yes
slave1: Warning: Permanently added 'slave1' (ECDSA) to the list of known hosts.
slave1: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave0.out
yes
^C
root@master:~# bash start-all.sh -y
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [master]
master: namenode running as process 987. Stop it first.
master: datanode running as process 1092. Stop it first.
slave3: datanode running as process 933. Stop it first.
slave1: datanode running as process 895. Stop it first.
^C
root@master:~# bash stop-all.sh -y
This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh
Stopping namenodes on [master]
master: stopping namenode
master: stopping datanode
slave1: stopping datanode
slave3: stopping datanode
^C
root@master:~# jps
1583 Jps
root@master:~# ssh root@192.168.1.5

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:49:36 2017 from master
root@slave2:~# raspi-config
Connection to 192.168.1.5 closed by remote host.
Connection to 192.168.1.5 closed.
root@master:~# nano /etc/hosts
root@master:~# ssh root@192.168.1.3

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:43:11 2017 from master
root@slave0:~# nano /etc/hosts
root@slave0:~# exit
logout
Connection to 192.168.1.3 closed.
root@master:~# ssh root@192.168.1.4

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:35:04 2017 from master
root@slave1:~# nano /etc/hosts
root@slave1:~# exit
logout
Connection to 192.168.1.4 closed.
root@master:~# ssh root@192.168.1.5

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:55:24 2017 from master
root@slave2:~# nano /etc/hosts
root@slave2:~# exit
logout
Connection to 192.168.1.5 closed.
root@master:~# nano /opt/hadoop/etc/hadoop/slaves
root@master:~# ssh root@192.168.1.3

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:56:48 2017 from master
root@slave0:~# nano /opt/hadoop/etc/hadoop/slaves
root@slave0:~# exit
logout
Connection to 192.168.1.3 closed.
root@master:~# ssh root@192.168.1.4

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:57:19 2017 from master
root@slave1:~# nano/opt/hadoop/etc/hadoop/slaves
-bash: nano/opt/hadoop/etc/hadoop/slaves: No such file or directory
root@slave1:~# nano /opt/hadoop/etc/hadoop/slaves
root@slave1:~# exit
logout
Connection to 192.168.1.4 closed.
root@master:~# ssh root@192.168.1.5

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:57:47 2017 from master
root@slave2:~# nano /opt/hadoop/etc/hadoop/slaves
root@slave2:~# nano /etc/hostname
root@slave2:~# exit
logout
Connection to 192.168.1.5 closed.
root@master:~# packet_write_wait: Connection to 192.168.1.2 port 22: Broken pipe
brandon-mitchells-computer:.ssh bmitchell$ ssh root@192.168.1.2
root@192.168.1.2's password: 

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:31:03 2017 from 192.168.1.6
root@master:~# bash start-all.sh -y
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [master]
master: starting namenode, logging to /opt/hadoop/logs/hadoop-root-namenode-master.out
The authenticity of host 'slave0 (192.168.1.3)' can't be established.
ECDSA key fingerprint is 93:4f:3d:cd:32:d1:6e:6c:6e:20:05:fb:a3:9d:47:99.
Are you sure you want to continue connecting (yes/no)? The authenticity of host 'slave2 (192.168.1.5)' can't be established.
ECDSA key fingerprint is 93:4f:3d:cd:32:d1:6e:6c:6e:20:05:fb:a3:9d:47:99.
Are you sure you want to continue connecting (yes/no)? master: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-master.out
slave1: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave1.out
yes
slave0: Warning: Permanently added 'slave0' (ECDSA) to the list of known hosts.
slave0: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave0.out
^Cslave2: Host key verification failed.

root@master:~# bash stop-all.sh 
This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh
Stopping namenodes on [master]
master: stopping namenode
The authenticity of host 'slave2 (192.168.1.5)' can't be established.
ECDSA key fingerprint is 93:4f:3d:cd:32:d1:6e:6c:6e:20:05:fb:a3:9d:47:99.
Are you sure you want to continue connecting (yes/no)? master: stopping datanode
slave1: stopping datanode
slave0: stopping datanode
yes
slave2: Warning: Permanently added 'slave2' (ECDSA) to the list of known hosts.
slave2: no datanode to stop
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: no secondarynamenode to stop
stopping yarn daemons
no resourcemanager to stop
master: no nodemanager to stop
slave1: no nodemanager to stop
slave0: no nodemanager to stop
slave2: no nodemanager to stop
no proxyserver to stop
root@master:~# bash start-all.sh
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [master]
master: starting namenode, logging to /opt/hadoop/logs/hadoop-root-namenode-master.out
master: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-master.out
slave0: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave0.out
slave2: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave2.out
slave1: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave1.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /opt/hadoop/logs/hadoop-root-secondarynamenode-master.out
starting yarn daemons
starting resourcemanager, logging to /opt/hadoop/logs/yarn-root-resourcemanager-master.out
master: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-master.out
slave1: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave1.out
slave2: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave2.out
slave0: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave0.out
root@master:~# jps
2243 NodeManager
1955 SecondaryNameNode
2135 ResourceManager
1787 DataNode
2541 Jps
1677 NameNode
root@master:~# ls -l
total 17872
drwxr-xr-x 15 root root     4096 Jan 26  2016 hadoop-2.7.2-src
-rw-r--r--  1 root root 18290860 Jan 26  2016 hadoop-2.7.2-src.tar.gz
drwxr-xr-x  2 root root     4096 Mar 26 07:43 hadoop_sample_txtfiles
root@master:~# cd 07:43 hadoop_sample_txtfiles
-bash: cd: 07:43: No such file or directory
root@master:~# cd hadoop_sample_txtfiles
root@master:~/hadoop_sample_txtfiles# ls -l
total 548292
-rw-r--r-- 1 root root 524288000 Mar 26 07:44 large_text_file
-rw-r--r-- 1 root root  35926176 Mar 26 07:15 mediumfile.txt
-rw-r--r-- 1 root root   1226438 Mar 26 07:15 smallfile.txt
root@master:~/hadoop_sample_txtfiles# hdfs dfs -put smallfile.txt /smallfile.txt
root@master:~/hadoop_sample_txtfiles# hdfs dfs -ls -R
ls: `.': No such file or directory
root@master:~/hadoop_sample_txtfiles# hdfs dfs -ls -R /
-rw-r--r--   4 root supergroup    1226438 2017-03-27 05:06 /smallfile.txt
root@master:~/hadoop_sample_txtfiles# cd ~
root@master:~# cd ~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target/
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# time hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /smallfile.txt /smallfile-out
17/03/27 05:08:10 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.2:8050
17/03/27 05:08:13 INFO input.FileInputFormat: Total input paths to process : 1
17/03/27 05:08:14 INFO mapreduce.JobSubmitter: number of splits:1
17/03/27 05:08:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490591081500_0001
17/03/27 05:08:16 INFO impl.YarnClientImpl: Submitted application application_1490591081500_0001
17/03/27 05:08:17 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1490591081500_0001/
17/03/27 05:08:17 INFO mapreduce.Job: Running job: job_1490591081500_0001
17/03/27 05:09:00 INFO mapreduce.Job: Job job_1490591081500_0001 running in uber mode : false
17/03/27 05:09:00 INFO mapreduce.Job:  map 0% reduce 0%
17/03/27 05:09:34 INFO mapreduce.Job:  map 67% reduce 0%
17/03/27 05:09:39 INFO mapreduce.Job:  map 100% reduce 0%
17/03/27 05:10:03 INFO mapreduce.Job:  map 100% reduce 50%
17/03/27 05:10:11 INFO mapreduce.Job:  map 100% reduce 100%
17/03/27 05:10:13 INFO mapreduce.Job: Job job_1490591081500_0001 completed successfully
17/03/27 05:10:14 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=289177
		FILE: Number of bytes written=930855
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1226536
		HDFS: Number of bytes written=213143
		HDFS: Number of read operations=9
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Job Counters 
		Killed reduce tasks=1
		Launched map tasks=1
		Launched reduce tasks=2
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=137084
		Total time spent by all reduces in occupied slots (ms)=92362
		Total time spent by all map tasks (ms)=34271
		Total time spent by all reduce tasks (ms)=46181
		Total vcore-milliseconds taken by all map tasks=137084
		Total vcore-milliseconds taken by all reduce tasks=184724
		Total megabyte-milliseconds taken by all map tasks=8773376
		Total megabyte-milliseconds taken by all reduce tasks=4710462
	Map-Reduce Framework
		Map input records=20997
		Map output records=212726
		Map output bytes=2054654
		Map output materialized bytes=289177
		Input split bytes=98
		Combine input records=212726
		Combine output records=19591
		Reduce input groups=19591
		Reduce shuffle bytes=289177
		Reduce input records=19591
		Reduce output records=19591
		Spilled Records=39182
		Shuffled Maps =2
		Failed Shuffles=0
		Merged Map outputs=2
		GC time elapsed (ms)=1736
		CPU time spent (ms)=20630
		Physical memory (bytes) snapshot=299380736
		Virtual memory (bytes) snapshot=759472128
		Total committed heap usage (bytes)=154406912
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1226438
	File Output Format Counters 
		Bytes Written=213143

real	2m10.388s
user	0m13.200s
sys	0m0.920s
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# cd ~
root@master:~# cd hadoop_sample_txtfiles
root@master:~/hadoop_sample_txtfiles# hdfs dfs -put mediumfile.txt /mediumfile.txt
root@master:~/hadoop_sample_txtfiles# cd ~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target/
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# time hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /mediumfile.txt /mediumfile-out
17/03/27 05:13:30 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.2:8050
17/03/27 05:13:33 INFO input.FileInputFormat: Total input paths to process : 1
17/03/27 05:13:33 INFO mapreduce.JobSubmitter: number of splits:7
17/03/27 05:13:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490591081500_0002
17/03/27 05:13:35 INFO impl.YarnClientImpl: Submitted application application_1490591081500_0002
17/03/27 05:13:35 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1490591081500_0002/
17/03/27 05:13:35 INFO mapreduce.Job: Running job: job_1490591081500_0002
17/03/27 05:14:00 INFO mapreduce.Job: Job job_1490591081500_0002 running in uber mode : false
17/03/27 05:14:00 INFO mapreduce.Job:  map 0% reduce 0%
17/03/27 05:14:19 INFO mapreduce.Job:  map 10% reduce 0%
17/03/27 05:14:28 INFO mapreduce.Job:  map 16% reduce 0%
17/03/27 05:14:29 INFO mapreduce.Job:  map 29% reduce 0%
17/03/27 05:14:32 INFO mapreduce.Job:  map 48% reduce 0%
17/03/27 05:14:33 INFO mapreduce.Job:  map 53% reduce 0%
17/03/27 05:14:35 INFO mapreduce.Job:  map 71% reduce 0%
17/03/27 05:14:52 INFO mapreduce.Job:  map 71% reduce 2%
17/03/27 05:14:57 INFO mapreduce.Job:  map 71% reduce 5%
17/03/27 05:15:07 INFO mapreduce.Job:  map 76% reduce 5%
17/03/27 05:15:08 INFO mapreduce.Job:  map 86% reduce 5%
17/03/27 05:15:09 INFO mapreduce.Job:  map 86% reduce 7%
17/03/27 05:15:10 INFO mapreduce.Job:  map 95% reduce 14%
17/03/27 05:15:11 INFO mapreduce.Job:  map 100% reduce 14%
17/03/27 05:15:13 INFO mapreduce.Job:  map 100% reduce 53%
17/03/27 05:15:16 INFO mapreduce.Job:  map 100% reduce 77%
17/03/27 05:15:18 INFO mapreduce.Job:  map 100% reduce 83%
17/03/27 05:15:19 INFO mapreduce.Job:  map 100% reduce 89%
17/03/27 05:15:22 INFO mapreduce.Job:  map 100% reduce 97%
17/03/27 05:15:23 INFO mapreduce.Job:  map 100% reduce 100%
17/03/27 05:15:24 INFO mapreduce.Job: Job job_1490591081500_0002 completed successfully
17/03/27 05:15:25 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=7692310
		FILE: Number of bytes written=16442529
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=35951445
		HDFS: Number of bytes written=3103134
		HDFS: Number of read operations=27
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Job Counters 
		Killed map tasks=1
		Launched map tasks=8
		Launched reduce tasks=2
		Data-local map tasks=8
		Total time spent by all maps in occupied slots (ms)=1826596
		Total time spent by all reduces in occupied slots (ms)=177238
		Total time spent by all map tasks (ms)=456649
		Total time spent by all reduce tasks (ms)=88619
		Total vcore-milliseconds taken by all map tasks=1826596
		Total vcore-milliseconds taken by all reduce tasks=354476
		Total megabyte-milliseconds taken by all map tasks=116902144
		Total megabyte-milliseconds taken by all reduce tasks=9039138
	Map-Reduce Framework
		Map input records=788346
		Map output records=6185757
		Map output bytes=59289268
		Map output materialized bytes=7692382
		Input split bytes=693
		Combine input records=6185757
		Combine output records=518987
		Reduce input groups=272380
		Reduce shuffle bytes=7692382
		Reduce input records=518987
		Reduce output records=272380
		Spilled Records=1037974
		Shuffled Maps =14
		Failed Shuffles=0
		Merged Map outputs=14
		GC time elapsed (ms)=8068
		CPU time spent (ms)=302530
		Physical memory (bytes) snapshot=1309147136
		Virtual memory (bytes) snapshot=2698981376
		Total committed heap usage (bytes)=888909824
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=35950752
	File Output Format Counters 
		Bytes Written=3103134

real	2m1.375s
user	0m12.770s
sys	0m0.740s
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# time hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /smallfile.txt /smallfile-out
17/03/27 05:15:53 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.2:8050
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://master:54310/smallfile-out already exists
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)
	at org.apache.hadoop.examples.WordCount.main(WordCount.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
	at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

real	0m7.753s
user	0m8.010s
sys	0m0.430s
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# cd ~
root@master:~# cd hadoop_sample_txtfiles
root@master:~/hadoop_sample_txtfiles# ls -l
total 548292
-rw-r--r-- 1 root root 524288000 Mar 26 07:44 large_text_file
-rw-r--r-- 1 root root  35926176 Mar 26 07:15 mediumfile.txt
-rw-r--r-- 1 root root   1226438 Mar 26 07:15 smallfile.txt
root@master:~/hadoop_sample_txtfiles# mv large_text_file large_text_file.txt
root@master:~/hadoop_sample_txtfiles# ls -l
total 548292
-rw-r--r-- 1 root root 524288000 Mar 26 07:44 large_text_file.txt
-rw-r--r-- 1 root root  35926176 Mar 26 07:15 mediumfile.txt
-rw-r--r-- 1 root root   1226438 Mar 26 07:15 smallfile.txt
root@master:~/hadoop_sample_txtfiles# hdfs dfs -put large_text_file.txt /large_text_file.txt
root@master:~/hadoop_sample_txtfiles# cd ~
root@master:~# cd ~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target/
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# time hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /large_text_file.txt /large_text_file-out
17/03/27 05:22:00 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.2:8050
17/03/27 05:22:04 INFO input.FileInputFormat: Total input paths to process : 1
17/03/27 05:22:05 INFO mapreduce.JobSubmitter: number of splits:100
17/03/27 05:22:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490591081500_0003
17/03/27 05:22:07 INFO impl.YarnClientImpl: Submitted application application_1490591081500_0003
17/03/27 05:22:08 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1490591081500_0003/
17/03/27 05:22:08 INFO mapreduce.Job: Running job: job_1490591081500_0003
17/03/27 05:22:41 INFO mapreduce.Job: Job job_1490591081500_0003 running in uber mode : false
17/03/27 05:22:41 INFO mapreduce.Job:  map 0% reduce 0%
17/03/27 05:23:33 INFO mapreduce.Job:  map 2% reduce 0%
17/03/27 05:23:33 INFO mapreduce.Job: Task Id : attempt_1490591081500_0003_m_000010_0, Status : FAILED
Exception from container-launch.
Container id: container_1490591081500_0003_01_000013
Exit code: 134
Exception message: /bin/bash: line 1:  3786 Aborted                 /usr/lib/jvm/jdk-8-oracle-arm32-vfp-hflt/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx204M -Djava.io.tmpdir=/hdfs/tmp/nm-local-dir/usercache/root/appcache/application_1490591081500_0003/container_1490591081500_0003_01_000013/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000013 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.1.5 33501 attempt_1490591081500_0003_m_000010_0 13 > /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000013/stdout 2> /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000013/stderr

Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1:  3786 Aborted                 /usr/lib/jvm/jdk-8-oracle-arm32-vfp-hflt/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx204M -Djava.io.tmpdir=/hdfs/tmp/nm-local-dir/usercache/root/appcache/application_1490591081500_0003/container_1490591081500_0003_01_000013/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000013 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.1.5 33501 attempt_1490591081500_0003_m_000010_0 13 > /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000013/stdout 2> /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000013/stderr

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:545)
	at org.apache.hadoop.util.Shell.run(Shell.java:456)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


Container exited with a non-zero exit code 134

17/03/27 05:23:33 INFO mapreduce.Job: Task Id : attempt_1490591081500_0003_m_000000_0, Status : FAILED
Exception from container-launch.
Container id: container_1490591081500_0003_01_000002
Exit code: 134
Exception message: /bin/bash: line 1:  3781 Aborted                 /usr/lib/jvm/jdk-8-oracle-arm32-vfp-hflt/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx204M -Djava.io.tmpdir=/hdfs/tmp/nm-local-dir/usercache/root/appcache/application_1490591081500_0003/container_1490591081500_0003_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.1.5 33501 attempt_1490591081500_0003_m_000000_0 2 > /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000002/stdout 2> /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000002/stderr

Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1:  3781 Aborted                 /usr/lib/jvm/jdk-8-oracle-arm32-vfp-hflt/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx204M -Djava.io.tmpdir=/hdfs/tmp/nm-local-dir/usercache/root/appcache/application_1490591081500_0003/container_1490591081500_0003_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.1.5 33501 attempt_1490591081500_0003_m_000000_0 2 > /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000002/stdout 2> /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000002/stderr

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:545)
	at org.apache.hadoop.util.Shell.run(Shell.java:456)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


Container exited with a non-zero exit code 134

17/03/27 05:23:34 INFO mapreduce.Job:  map 0% reduce 0%
17/03/27 05:24:35 INFO mapreduce.Job:  map 2% reduce 0%
17/03/27 05:24:36 INFO mapreduce.Job:  map 3% reduce 0%
17/03/27 05:24:44 INFO mapreduce.Job:  map 4% reduce 0%
17/03/27 05:24:45 INFO mapreduce.Job:  map 5% reduce 0%
17/03/27 05:24:46 INFO mapreduce.Job:  map 8% reduce 0%
17/03/27 05:24:48 INFO mapreduce.Job:  map 9% reduce 0%
17/03/27 05:25:02 INFO mapreduce.Job:  map 10% reduce 0%
17/03/27 05:25:04 INFO mapreduce.Job: Task Id : attempt_1490591081500_0003_m_000000_1, Status : FAILED
Error: Java heap space
Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

17/03/27 05:25:05 INFO mapreduce.Job:  map 9% reduce 0%
17/03/27 05:25:11 INFO mapreduce.Job:  map 9% reduce 2%
17/03/27 05:25:26 INFO mapreduce.Job:  map 9% reduce 3%
17/03/27 05:25:39 INFO mapreduce.Job:  map 11% reduce 3%
17/03/27 05:25:42 INFO mapreduce.Job:  map 11% reduce 4%
17/03/27 05:25:47 INFO mapreduce.Job: Task Id : attempt_1490591081500_0003_m_000014_0, Status : FAILED
Exception from container-launch.
Container id: container_1490591081500_0003_01_000021
Exit code: 134
Exception message: /bin/bash: line 1:  2078 Aborted                 /usr/lib/jvm/jdk-8-oracle-arm32-vfp-hflt/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx204M -Djava.io.tmpdir=/hdfs/tmp/nm-local-dir/usercache/root/appcache/application_1490591081500_0003/container_1490591081500_0003_01_000021/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000021 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.1.5 33501 attempt_1490591081500_0003_m_000014_0 21 > /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000021/stdout 2> /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000021/stderr

Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1:  2078 Aborted                 /usr/lib/jvm/jdk-8-oracle-arm32-vfp-hflt/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx204M -Djava.io.tmpdir=/hdfs/tmp/nm-local-dir/usercache/root/appcache/application_1490591081500_0003/container_1490591081500_0003_01_000021/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000021 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.1.5 33501 attempt_1490591081500_0003_m_000014_0 21 > /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000021/stdout 2> /opt/hadoop/logs/userlogs/application_1490591081500_0003/container_1490591081500_0003_01_000021/stderr

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:545)
	at org.apache.hadoop.util.Shell.run(Shell.java:456)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


Container exited with a non-zero exit code 134

17/03/27 05:26:05 INFO mapreduce.Job:  map 12% reduce 4%
17/03/27 05:26:07 INFO mapreduce.Job: Task Id : attempt_1490591081500_0003_m_000000_2, Status : FAILED
Error: Java heap space
17/03/27 05:26:08 INFO mapreduce.Job:  map 11% reduce 4%
17/03/27 05:26:19 INFO mapreduce.Job:  map 12% reduce 4%
17/03/27 05:26:25 INFO mapreduce.Job:  map 13% reduce 4%
17/03/27 05:26:27 INFO mapreduce.Job:  map 14% reduce 4%
17/03/27 05:26:28 INFO mapreduce.Job:  map 15% reduce 5%
17/03/27 05:26:29 INFO mapreduce.Job:  map 16% reduce 5%
17/03/27 05:26:37 INFO mapreduce.Job:  map 17% reduce 5%
17/03/27 05:26:40 INFO mapreduce.Job:  map 17% reduce 6%
17/03/27 05:26:51 INFO mapreduce.Job:  map 18% reduce 6%
17/03/27 05:26:59 INFO mapreduce.Job:  map 19% reduce 6%
17/03/27 05:27:00 INFO mapreduce.Job:  map 100% reduce 100%
17/03/27 05:27:04 INFO mapreduce.Job: Job job_1490591081500_0003 failed with state FAILED due to: Task failed task_1490591081500_0003_m_000000
Job failed as tasks failed. failedMaps:1 failedReduces:0

17/03/27 05:27:05 INFO mapreduce.Job: Counters: 41
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=2116431
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=8514438992
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=54
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Failed map tasks=6
		Killed map tasks=81
		Killed reduce tasks=2
		Launched map tasks=30
		Launched reduce tasks=2
		Other local map tasks=5
		Data-local map tasks=25
		Total time spent by all maps in occupied slots (ms)=9824752
		Total time spent by all reduces in occupied slots (ms)=526724
		Total time spent by all map tasks (ms)=2456188
		Total time spent by all reduce tasks (ms)=263362
		Total vcore-milliseconds taken by all map tasks=9824752
		Total vcore-milliseconds taken by all reduce tasks=1053448
		Total megabyte-milliseconds taken by all map tasks=628784128
		Total megabyte-milliseconds taken by all reduce tasks=26862924
	Map-Reduce Framework
		Map input records=0
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=216
		Input split bytes=1872
		Combine input records=0
		Combine output records=0
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=13545
		CPU time spent (ms)=1455460
		Physical memory (bytes) snapshot=2993123328
		Virtual memory (bytes) snapshot=5820059648
		Total committed heap usage (bytes)=2184781824
	File Input Format Counters 
		Bytes Read=8514437120

real	5m11.250s
user	0m14.920s
sys	0m1.050s
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# time hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /smallfile.txt /smallfile-out
17/03/27 05:48:03 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.2:8050
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://master:54310/smallfile-out already exists
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)
	at org.apache.hadoop.examples.WordCount.main(WordCount.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
	at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

real	0m8.494s
user	0m8.170s
sys	0m0.440s
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# cd ~
root@master:~# bash stop-all.sh
This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh
Stopping namenodes on [master]
master: stopping namenode
master: stopping datanode
slave2: stopping datanode
slave1: stopping datanode
slave0: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode
stopping yarn daemons
stopping resourcemanager
master: stopping nodemanager
slave2: stopping nodemanager
slave1: stopping nodemanager
slave0: stopping nodemanager
master: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
slave2: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
slave1: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
slave0: nodemanager did not stop gracefully after 5 seconds: killing with kill -9
no proxyserver to stop
root@master:~# ssh root@192.168.1.5

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 05:00:17 2017 from master
root@slave2:~# shutdown now
Connection to 192.168.1.5 closed by remote host.
Connection to 192.168.1.5 closed.
root@master:~# ssh root@192.168.1.4

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:59:43 2017 from master
root@slave1:~# shutdown now
Connection to 192.168.1.4 closed by remote host.
Connection to 192.168.1.4 closed.
root@master:~# ssh root@192.168.1.3

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 04:59:06 2017 from master
root@slave0:~# shutdown now
Connection to 192.168.1.3 closed by remote host.
Connection to 192.168.1.3 closed.
root@master:~# ssh root@192.168.1.2

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 05:02:03 2017 from 192.168.1.6
root@master:~# exit
logout
Connection to 192.168.1.2 closed.
root@master:~# shutdown now
Connection to 192.168.1.2 closed by remote host.
Connection to 192.168.1.2 closed.
brandon-mitchells-computer:.ssh bmitchell$ ssh root@192.168.1.2
^C
brandon-mitchells-computer:.ssh bmitchell$ ssh root@192.168.1.2
^C
brandon-mitchells-computer:.ssh bmitchell$ ssh root@192.168.1.2
root@192.168.1.2's password: 

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 05:50:30 2017 from master
root@master:~# bash start-all.sh
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [master]
master: starting namenode, logging to /opt/hadoop/logs/hadoop-root-namenode-master.out
master: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-master.out
slave0: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave0.out
slave1: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave1.out
slave2: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave2.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /opt/hadoop/logs/hadoop-root-secondarynamenode-master.out
starting yarn daemons
starting resourcemanager, logging to /opt/hadoop/logs/yarn-root-resourcemanager-master.out
master: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-master.out
slave2: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave2.out
slave0: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave0.out
slave1: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave1.out
root@master:~# jps
1363 DataNode
1781 NodeManager
1509 SecondaryNameNode
1255 NameNode
1672 ResourceManager
2157 Jps
root@master:~# ls -l
total 17872
drwxr-xr-x 15 root root     4096 Jan 26  2016 hadoop-2.7.2-src
-rw-r--r--  1 root root 18290860 Jan 26  2016 hadoop-2.7.2-src.tar.gz
drwxr-xr-x  2 root root     4096 Mar 27 05:17 hadoop_sample_txtfiles
root@master:~# cd hadoop_sample_txtfiles
root@master:~/hadoop_sample_txtfiles# mkfile -n 100m 100MB.txt
-bash: mkfile: command not found
root@master:~/hadoop_sample_txtfiles# apt-get mkfile
E: Invalid operation mkfile
root@master:~/hadoop_sample_txtfiles# apt-get install mkfile
Reading package lists... Done
Building dependency tree        
Reading state information... Done
E: Unable to locate package mkfile
root@master:~/hadoop_sample_txtfiles# ls -l
total 548292
-rw-r--r-- 1 root root 524288000 Mar 26 07:44 large_text_file.txt
-rw-r--r-- 1 root root  35926176 Mar 26 07:15 mediumfile.txt
-rw-r--r-- 1 root root   1226438 Mar 26 07:15 smallfile.txt
root@master:~/hadoop_sample_txtfiles# jps
2449 Jps
1363 DataNode
1781 NodeManager
1509 SecondaryNameNode
1255 NameNode
1672 ResourceManager
root@master:~/hadoop_sample_txtfiles# bash stop-all.sh
This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh
Stopping namenodes on [master]
master: stopping namenode
master: stopping datanode
slave0: stopping datanode
slave2: stopping datanode
slave1: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode
stopping yarn daemons
stopping resourcemanager
master: stopping nodemanager
slave1: stopping nodemanager
slave0: stopping nodemanager
slave2: stopping nodemanager
no proxyserver to stop
root@master:~/hadoop_sample_txtfiles# ssh root@192.168.1.5

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 05:50:06 2017 from master
root@slave2:~# shutdown now
Connection to 192.168.1.5 closed by remote host.
Connection to 192.168.1.5 closed.
root@master:~/hadoop_sample_txtfiles# ssh root@192.168.1.4

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 05:50:15 2017 from master
root@slave1:~# shutdown now
Connection to 192.168.1.4 closed by remote host.
Connection to 192.168.1.4 closed.
root@master:~/hadoop_sample_txtfiles# ssh root@192.168.1.3

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 05:50:22 2017 from master
root@slave0:~# shutdown now
Connection to 192.168.1.3 closed by remote host.
Connection to 192.168.1.3 closed.
root@master:~/hadoop_sample_txtfiles# shutdown now
Connection to 192.168.1.2 closed by remote host.
Connection to 192.168.1.2 closed.
brandon-mitchells-computer:.ssh bmitchell$ who
bmitchell console  Mar 19 19:33 
bmitchell ttys000  Mar 26 22:48 
brandon-mitchells-computer:.ssh bmitchell$ which
usage: which [-as] program ...
brandon-mitchells-computer:.ssh bmitchell$ df
Filesystem                                         512-blocks       Used Available Capacity  iused      ifree %iused  Mounted on
/dev/disk1                                         1951825920 1359401232 591912688    70%  2487625 4292479654    0%   /
devfs                                                     364        364         0   100%      630          0  100%   /dev
map -hosts                                                  0          0         0   100%        0          0  100%   /net
map auto_home                                               0          0         0   100%        0          0  100%   /home
//guest:@localhost:53941/%5BC%5D%20Windows%207%201  267714552  115993744 151720808    44% 14499216   18965101   43%   /Volumes/C
brandon-mitchells-computer:.ssh bmitchell$ df -H
Filesystem                                           Size   Used  Avail Capacity  iused      ifree %iused  Mounted on
/dev/disk1                                           999G   696G   303G    70%  2487624 4292479655    0%   /
devfs                                                186k   186k     0B   100%      630          0  100%   /dev
map -hosts                                             0B     0B     0B   100%        0          0  100%   /net
map auto_home                                          0B     0B     0B   100%        0          0  100%   /home
//guest:@localhost:53941/%5BC%5D%20Windows%207%201   137G    59G    78G    44% 14499145   18965172   43%   /Volumes/C
brandon-mitchells-computer:.ssh bmitchell$ printenv
TERM_PROGRAM=Apple_Terminal
SHELL=/bin/bash
TERM=xterm-256color
TMPDIR=/var/folders/g3/yh1hl4qx2j93vqj40hv6k50r0000gn/T/
Apple_PubSub_Socket_Render=/private/tmp/com.apple.launchd.opBRMBpE6y/Render
TERM_PROGRAM_VERSION=388
OLDPWD=/Users/bmitchell
TERM_SESSION_ID=2B579332-D467-45A1-97DC-EE9A63F88654
USER=bmitchell
SSH_AUTH_SOCK=/private/tmp/com.apple.launchd.dwVb3CCSGJ/Listeners
__CF_USER_TEXT_ENCODING=0x1F5:0x0:0x0
PATH=/Users/bmitchell/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin
PWD=/Users/bmitchell/.ssh
LANG=en_US.UTF-8
XPC_FLAGS=0x0
XPC_SERVICE_NAME=0
SHLVL=1
HOME=/Users/bmitchell
LOGNAME=bmitchell
DISPLAY=/private/tmp/com.apple.launchd.xOoAMRXFXo/org.macosforge.xquartz:0
SECURITYSESSIONID=186a6
_=/usr/bin/printenv
brandon-mitchells-computer:.ssh bmitchell$ ssh root@192.168.1.2
root@192.168.1.2's password: 

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Mar 27 06:08:54 2017 from 192.168.1.6
root@master:~# ls -l
total 17872
drwxr-xr-x 15 root root     4096 Jan 26  2016 hadoop-2.7.2-src
-rw-r--r--  1 root root 18290860 Jan 26  2016 hadoop-2.7.2-src.tar.gz
drwxr-xr-x  2 root root     4096 Mar 27 05:17 hadoop_sample_txtfiles
root@master:~# ~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/t^Cget/
root@master:~# cd ~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target/
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# time hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /smallfile.txt /smallfile-out
17/03/27 06:54:35 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.2:8050
java.net.ConnectException: Call From master/192.168.1.2 to master:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1424)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:145)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)
	at org.apache.hadoop.examples.WordCount.main(WordCount.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
	at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 41 more

real	0m11.065s
user	0m8.100s
sys	0m0.530s
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# bash start-all.sh
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [master]
master: starting namenode, logging to /opt/hadoop/logs/hadoop-root-namenode-master.out
master: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-master.out
slave1: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave1.out
slave2: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave2.out
slave0: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-slave0.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /opt/hadoop/logs/hadoop-root-secondarynamenode-master.out
starting yarn daemons
starting resourcemanager, logging to /opt/hadoop/logs/yarn-root-resourcemanager-master.out
master: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-master.out
slave0: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave0.out
slave1: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave1.out
slave2: starting nodemanager, logging to /opt/hadoop/logs/yarn-root-nodemanager-slave2.out
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# time hadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /smallfile.txt /smallfile-out
17/03/27 06:58:13 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.2:8050
java.net.ConnectException: Call From master/192.168.1.2 to master:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1424)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:145)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)
	at org.apache.hadoop.examples.WordCount.main(WordCount.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
	at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 41 more

real	0m7.511s
user	0m7.810s
sys	0m0.420s
root@master:~/hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/target# packet_write_wait: Connection to 192.168.1.2 port 22: Broken pipe
brandon-mitchells-computer:.ssh bmitchell$ ssh root@192.168.1.2
ssh: connect to host 192.168.1.2 port 22: Operation timed out
brandon-mitchells-computer:.ssh bmitchell$ cd ~
brandon-mitchells-computer:~ bmitchell$ ssh cloudera@127.0.0.1 -p 7180
ssh_exchange_identification: Connection closed by remote host
brandon-mitchells-computer:~ bmitchell$ cd .ssh
brandon-mitchells-computer:.ssh bmitchell$ nano known_hosts
brandon-mitchells-computer:.ssh bmitchell$ ssh cloudera@127.0.0.1 -p 2222
ssh: connect to host 127.0.0.1 port 2222: Connection refused
brandon-mitchells-computer:.ssh bmitchell$ 
